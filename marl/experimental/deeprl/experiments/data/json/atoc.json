{"__doc__":"config for ATOC on the spread particles environment",
  "actor_a_output_act":"tanh",
  "atoc_comm":"marl.experimental.deeprl.policies.atoc_comm.bidirectional_lstm_batched.BidirectionalLSTMBatched",
  "atoc_critic_sizes":[100,50],
  "agent_density": 4,
  "batch_size":512,
  "buff_min_items_before_learn":100,
  "buff_seq_length":10,
  "buffer_size":100000,
  "classifier_batch_size":256,
  "classifier_buffer_size":100000,
  "classifier_hidden_sizes":[128,64],
  "classifier_lr":0.001,
  "comm_bandwidth":3,
  "comm_decision_period":15,
  "communication_channel_sizes":[64,64],
  "critic_lr":0.005,
  "disable_communication":true,
  "env_name":"multiagent",
  "eps_decay_slope":2,
  "eps_num_annealings":1,
  "epsilon":0.99,
  "exploration":"marl.experimental.deeprl.exploration.exploration_ou.ExplorationOu",
  "force_communication":false,
  "force_cpu":false,
  "gamma":0.9,
  "hidden_sizes":[128,64,32,32],
  "independent_policy":"marl.experimental.deeprl.policies.ddpg_policy_batched.DDPGPolicyBatched",
  "learning_period":100,
  "lr":0.001,
  "mu":0,
  "network":"marl.experimental.deeprl.policies.networks.SimpleFF",
  "num_agents":4,
  "num_landmarks":4,
  "num_learning_iterations":10,
  "num_logs":1000,
  "num_perceived_agents":3,
  "num_perceived_landmarks":4,
  "num_serializations":10,
  "num_steps":1000000,
  "output_activation":"tanh",
  "output_rescale":null,
  "policy":"marl.experimental.deeprl.policies.atoc_policy.ATOCPolicy",
  "render":false,
  "reward_last_n_steps":30,
  "reward_scale":1,
  "rollout_length":100,
  "scenario_name":"custom/custom_spread",
  "show_agent_velocities":true,
  "sigma":0.55,
  "sigmoid":false,
  "sleep":0.01,
  "tau":0.01,
  "theta":0.25}